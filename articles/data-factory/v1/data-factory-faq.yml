### YamlMime:FAQ
metadata:
  title: Azure Data Factory – Häufig gestellte Fragen
  description: Häufig gestellte Fragen zu Azure Data Factory
  author: dcstwh
  ms.author: weetok
  ms.reviewer: jburchel
  ms.service: data-factory
  ms.topic: conceptual
  ms.date: 01/10/2018
  ms.openlocfilehash: 2d5e67f4a3035d152db65760ee09a218f0c8688f
  ms.sourcegitcommit: 23040f695dd0785409ab964613fabca1645cef90
  ms.translationtype: HT
  ms.contentlocale: de-DE
  ms.lasthandoff: 06/14/2021
  ms.locfileid: "112063613"
title: Azure Data Factory – Häufig gestellte Fragen
summary: >
  > [!NOTE]

  > Dieser Artikel gilt für Version 1 von Data Factory. Bei Verwendung der aktuellen Version des Data Factory-Diensts finden Sie weitere Informationen unter [frequently asked question - Data Factory](../frequently-asked-questions.yml) (Häufig gestellte Fragen: Data Factory).


  [!INCLUDE [updated-for-az](../../../includes/updated-for-az.md)]
sections:
- name: Allgemeine Fragen
  questions:
  - question: >
      Was ist Azure Data Factory?
    answer: "Data Factory ist ein cloudbasierter Datenintegrationsdienst, der das **Verschieben und Transformieren von Daten automatisiert**. Genau wie ein Betrieb, in dem Anlagen Rohmaterialien verarbeiten und in Endprodukte umwandeln, organisiert Data Factory vorhandene Dienste so, dass Rohdaten gesammelt und in nutzbare Informationen transformiert werden.\n\nData Factory ermöglicht das Erstellen von datengesteuerten Workflows zum Verschieben von Daten zwischen lokalen und cloudbasierten Datenspeichern sowie zum Verarbeiten/Transformieren von Daten mithilfe von Computediensten wie Azure HDInsight und Azure Data Lake Analytics. Nachdem Sie eine Pipeline erstellt haben, die die gewünschte Aktion ausführt, können Sie die regelmäßige Ausführung der Pipeline planen (stündlich, täglich, wöchentlich usw.).   \n\nWeitere Informationen finden Sie unter [Übersicht und wichtige Konzepte](data-factory-introduction.md).\n"
  - question: >
      Wo finde ich Preisinformationen zu Azure Data Factory?
    answer: "Preisinformationen zu Azure Data Factory finden Sie auf der Seite [Data Factory – Preisübersicht](https://go.microsoft.com/fwlink/?LinkId=517777).  \n"
  - question: >
      F: Was sind die ersten Schritte mit Azure Data Factory?
    answer: >
      * Eine Übersicht über Azure Data Factory finden Sie unter [Einführung in Azure Data Factory](data-factory-introduction.md).

      * Ein Tutorial zum **Kopieren/Verschieben von Daten** mit der Kopieraktivität finden Sie unter [Kopieren von Daten aus Azure Blob Storage in Azure SQL-Datenbank](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).

      * Ein Tutorial zum **Transformieren von Daten** mit der HDInsight Hive-Aktivität finden Sie unter [Verarbeiten von Daten durch Ausführen eines Hive-Skripts in einem Hadoop-Cluster](data-factory-build-your-first-pipeline.md).
  - question: >
      In welchen Regionen ist Data Factory verfügbar?
    answer: >
      Data Factory ist in den Regionen **USA, Westen** sowie in **Europa, Norden** verfügbar. Die von Data Factory verwendeten Rechen- und Speicherdienste können in anderen Regionen verfügbar sein. Siehe [Unterstützte Regionen](data-factory-introduction.md#supported-regions).
  - question: >
      Welche Grenzwerte sind hinsichtlich der Anzahl der Data Factorys/Pipelines/Aktivitäten/Datasets gegeben?
    answer: >
      Weitere Informationen finden Sie im Abschnitt **Einschränkungen von Azure Data Factory** unter dem Artikel [Begrenzungen, Kontingente und Einschränkungen von Azure-Abonnements und -Diensten](../../azure-resource-manager/management/azure-subscription-service-limits.md#data-factory-limits) .
  - question: >
      Was sieht die Erstellung-/Entwicklungsumgebung im Azure Data Factory-Dienst aus?
    answer: >
      Sie können Data Factorys mithilfe eines der folgenden Werkzeuge/SDKs erstellen:


      * **Visual Studio** : Sie können mit Visual Studio eine Azure Data Factory erstellen. Unter [Erstellen der ersten Azure Data Factory-Pipeline mit Visual Studio](data-factory-build-your-first-pipeline-using-vs.md) finden Sie weitere Informationen.

      * **Azure PowerShell** : Unter [Erstellen der ersten Azure Data Factory mit Azure PowerShell](data-factory-build-your-first-pipeline-using-powershell.md) finden Sie ein Tutorial und eine exemplarische Vorgehensweise zum Erstellen einer Data Factory mithilfe von PowerShell. In der [Data Factory-Cmdlet-Referenz](/powershell/module/az.datafactory/) in der MSDN-Bibliothek finden Sie eine umfassende Dokumentation zu Data Factory-Cmdlets.

      * **.NET-Klassenbibliothek** Sie können Data Factorys mithilfe des Data Factory .NET SDK programmgesteuert erstellen. Unter [Erstellen, Überwachen und Verwalten von Daten Factorys mit dem .NET SDK](data-factory-create-data-factories-programmatically.md) finden Sie eine exemplarische Vorgehensweise zum Erstellen einer Data Factory mit dem .NET SDK. In der [Data Factory-Klassenbibliotheksreferenz](/dotnet/api/microsoft.azure.management.datafactories.models) finden Sie eine umfassende Dokumentation zum Data Factory .NET SDK.

      * **REST-API** Sie können auch die vom Azure-Data Factory-Dienst verfügbar gemachte REST-API zum Erstellen und Bereitstellen von Data Factorys nutzen. In der [Data Factory-REST-API-Referenz](/rest/api/datafactory/) finden Sie eine umfassende Dokumentation zur Data Factory-REST-API.

      * **Azure Resource Manager-Vorlage** Weitere Informationen finden Sie unter [Tutorial: Erstellen der ersten Azure Data Factory mit einer Azure Resource Manager-Vorlage](data-factory-build-your-first-pipeline-using-arm.md).
  - question: >
      Können Data Factorys umbenannt werden?
    answer: >
      Nein. Wie bei anderen Azure-Ressourcen auch kann der Name einer Azure Data Factory nicht geändert werden.
  - question: >
      Kann ich eine Data Factory aus einem Azure-Abonnement in ein anderes verschieben?
    answer: >
      Ja. Verwenden Sie die Schaltfläche **Verschieben** auf Ihrem Data Factory-Blatt, wie im folgenden Diagramm dargestellt:


      ![Data Factory verschieben](media/data-factory-faq/move-data-factory.png)
  - question: >
      Welche Compute-Umgebungen werden von Data Factory unterstützt?
    answer: >
      Die folgende Tabelle enthält eine Liste von Compute-Umgebungen, die von Data Factory unterstützt werden, und die Aktivitäten, die darin ausgeführt werden können.


      | Compute-Umgebung | activities |

      | --- | --- |

      | [Bedarfsgesteuerter HDInsight-Cluster](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service) oder [Eigener HDInsight-Cluster](data-factory-compute-linked-services.md#azure-hdinsight-linked-service) |[DotNet](data-factory-use-custom-activities.md), [Hive](data-factory-hive-activity.md), [Pig](data-factory-pig-activity.md), [MapReduce](data-factory-map-reduce.md), [Hadoop Streaming](data-factory-hadoop-streaming-activity.md) |

      | [Azure Batch](data-factory-compute-linked-services.md#azure-batch-linked-service) |[DotNet](data-factory-use-custom-activities.md) |

      | [Azure Machine Learning Studio (klassisch)](data-factory-compute-linked-services.md#azure-machine-learning-studio-classic-linked-service) |[Aktivitäten in Studio (klassisch): Batchausführung und Ressourcenaktualisierung](data-factory-azure-ml-batch-execution-activity.md) |

      | [Azure Data Lake Analytics](data-factory-compute-linked-services.md#azure-data-lake-analytics-linked-service) |[Data Lake Analytics U-SQL](data-factory-usql-activity.md) |

      | [Azure SQL](data-factory-compute-linked-services.md#azure-sql-linked-service), [Azure Synapse Analytics](data-factory-compute-linked-services.md#azure-synapse-analytics-linked-service), [SQL Server](data-factory-compute-linked-services.md#sql-server-linked-service) |[Gespeicherte Prozedur](data-factory-stored-proc-activity.md) |
  - question: "Wie unterscheidet sich Azure Data Factory von SQL Server Integration Services (SSIS)? \n"
    answer: "Siehe die Präsentation [Azure Data Factory vs. SSIS](https://www.sqlbits.com/Sessions/Event15/Azure_Data_Factory_vs_SSIS) (in englischer Sprache) von einem unserer MVPs (Most Valued Professionals): Reza Rad. Einige der kürzlich in Data Factory vorgenommenen Änderungen werden möglicherweise in der Präsentation nicht aufgeführt. Azure Data Factory werden fortlaufend weitere Funktionen hinzugefügt. Azure Data Factory werden fortlaufend weitere Funktionen hinzugefügt. Diese Aktualisierungen werden irgendwann im Laufe dieses Jahres in den Vergleich der Datenintegrationstechnologien von Microsoft aufgenommen.   \n"
- name: Aktivitäten – Häufig gestellte Fragen
  questions:
  - question: >
      Welche verschiedenen Arten von Aktivitäten können in einer Data Factory-Pipeline verwendet werden?
    answer: >
      * [Datenverschiebungsaktivitäten](data-factory-data-movement-activities.md) zum Verschieben von Daten.

      * [Datentransformationsaktivitäten](data-factory-data-transformation-activities.md) zum Verarbeiten/Transformieren von Daten.
  - question: >
      Wann wird eine Aktivität ausgeführt?
    answer: >
      Die Konfigurationseinstellung **availability** in der Ausgabedatentabelle bestimmt, wann die Aktivität erfolgt. Wenn Eingabedatasets angegeben sind, prüft die Aktivität, ob alle Eingabedatenabhängigkeiten erfüllt sind (den Status **Bereit** aufweisen), bevor die Ausführung beginnt.
- name: Kopieraktivität – Häufig gestellte Fragen
  questions:
  - question: >
      Ist es besser, eine Pipeline mit mehreren Aktivitäten oder eine separate Pipeline für jede Aktivität einzurichten?
    answer: >
      Pipelines dienen zum Bündeln verwandter Aktivitäten. Sie können die Aktivitäten in einer Pipeline halten, wenn die Datasets, die diese verbinden, nicht von anderen Aktivitäten außerhalb der Pipeline genutzt werden. Auf diese Weise müssen Sie Pipelineaktivitäten nicht verknüpfen, damit diese sich aneinander ausrichten. Darüber hinaus kann die Datenintegrität in den Tabellen, die für die Pipeline intern sind, beim Aktualisieren der Pipeline besser beibehalten werden. Bei einer Pipelineaktualisierung werden alle Aktivitäten in der Pipeline beendet, entfernt und neu erstellt. Aus Erstellungssicht kann es auch einfacher sein, den Datenfluss innerhalb der zugehörigen Aktivitäten in einer JSON-Datei für die Pipeline nachzuverfolgen.
  - question: >
      Welche Datenspeicher werden unterstützt?
    answer: >
      Die Kopieraktivität in Data Factory kopiert die Daten aus einem Quelldatenspeicher in einen Senkendatenspeicher. Data Factory unterstützt die folgenden Datenspeicher. Daten aus beliebigen Quellen können in beliebige Senken geschrieben werden. Klicken Sie auf einen Datenspeicher, um zu erfahren, wie Daten in diesen/aus diesem Speicher kopiert werden.


      [!INCLUDE [data-factory-supported-data-stores](includes/data-factory-supported-data-stores.md)]


      > [!NOTE]

      > Datenspeicher mit * können lokal oder in Azure IaaS verfügbar sein. Für ihre Verwendung müssen Sie das [Datenverwaltungsgateway](data-factory-data-management-gateway.md) auf einem lokalen oder einem Azure IaaS-Computer installieren.
  - question: >
      Welche Dateiformate werden unterstützt?
    answer: >
      Azure Data Factory unterstützt die folgenden Dateiformattypen:


      * [Textformat](data-factory-supported-file-and-compression-formats.md#text-format)

      * [JSON-Format](data-factory-supported-file-and-compression-formats.md#json-format)

      * [Avro-Format](data-factory-supported-file-and-compression-formats.md#avro-format)

      * [ORC-Format](data-factory-supported-file-and-compression-formats.md#orc-format)

      * [Parquet-Format](data-factory-supported-file-and-compression-formats.md#parquet-format)
  - question: >
      Wo wird der Kopiervorgang ausgeführt?
    answer: >
      Ausführliche Informationen finden Sie im Abschnitt [Global verfügbare Datenverschiebung](data-factory-data-movement-activities.md#global) . Kurz gesagt: Wenn ein lokaler Datenspeicher beteiligt ist, wird der Kopiervorgang vom Datenverwaltungsgateway in Ihrer lokalen Umgebung ausgeführt. Wenn Daten zwischen zwei Cloudspeichern bewegt werden, wird der Kopiervorgang in der Region ausgeführt, die dem Standort der Senke in der gleichen geografischen Region am nächsten liegt.
- name: HDInsight-Aktivität – Häufig gestellte Fragen
  questions:
  - question: >
      In welchen Regionen wird HDInsight unterstützt?
    answer: >
      Weitere Informationen finden Sie im Abschnitt zur geografischen Verfügbarkeit des folgenden Artikels oder unter [HDInsight – Preisübersicht](https://azure.microsoft.com/pricing/details/hdinsight/).
  - question: >
      Welche Region wird von einem bedarfsgesteuerten HDInsight-Cluster verwendet?
    answer: "Der bedarfsgesteuerte HDInsight-Cluster wird in derselben Region erstellt, in der sich der Speicher befindet, den Sie für die Verwendung mit dem Cluster angegeben haben.    \n"
  - question: >
      Wie können weitere Speicherkonten mit Ihrem HDInsight-Cluster verknüpft werden?
    answer: >
      Wenn Sie Ihren eigenen HDInsight-Cluster (BYOC - Bring Your Own Cluster) verwenden, lesen Sie die folgenden Themen:


      * [Verwenden eines HDInsight-Clusters mit alternativen Speicherkonten und Metastores](https://social.technet.microsoft.com/wiki/contents/articles/23256.using-an-hdinsight-cluster-with-alternate-storage-accounts-and-metastores.aspx)

      * [Verwenden zusätzlicher Speicherkonten mit HDInsight Hive](/archive/blogs/cindygross/use-additional-storage-accounts-with-hdinsight-hive)


      Wenn Sie einen bedarfsgesteuerten Cluster verwenden, der vom Data Factory-Dienst erstellt wird, geben Sie zusätzliche Speicherkonten für den verknüpften HDInsight-Dienst an, damit der Data Factory-Dienst diese in Ihrem Auftrag registrieren kann. Verwenden Sie in der JSON-Definition des bedarfsgesteuerten verknüpften Diensts die **additionalLinkedServiceNames** -Eigenschaft, um alternative Speicherkonten anzugeben, wie im folgenden JSON-Codeausschnitt gezeigt:


      ```JSON

      {
          "name": "MyHDInsightOnDemandLinkedService",
          "properties":
          {
              "type": "HDInsightOnDemandLinkedService",
              "typeProperties": {
                  "version": "3.5",
                  "clusterSize": 1,
                  "timeToLive": "00:05:00",
                  "osType": "Linux",
                  "linkedServiceName": "LinkedService-SampleData",
                  "additionalLinkedServiceNames": [ "otherLinkedServiceName1", "otherLinkedServiceName2" ]
              }
          }
      }

      ```

      Im obigen Beispiel stellen "otherLinkedServiceName1" und "otherLinkedServiceName2" verknüpfte Dienste dar, deren Definitionen Anmeldeinformationen enthalten, die der HDInsight-Cluster für den Zugriff auf alternative Speicherkonten benötigt.
- name: Slices – Häufig gestellte Fragen
  questions:
  - question: >
      Wieso weisen meine Eingabeslices nicht den Status „Bereit“ auf?
    answer: "Ein weit verbreiteter Fehler besteht darin, die **external**-Eigenschaft im Eingabedataset nicht auf **true** festzulegen, wenn die Eingabedaten für die Data Factory extern sind (also nicht von der Data Factory erstellt wurden).\n\nIm folgenden Beispiel müssen Sie nur für **dataset1** den Wert **external** auf „true“ festlegen.  \n\n**DataFactory1** Pipeline 1: dataset1 &gt; activity1 &gt; dataset2 &gt; activity2 &gt; dataset3 Pipeline 2: dataset3 &gt; activity3 &gt; dataset4\n\nWenn Sie über eine andere Data Factory mit einer Pipeline, die Dataset4 nimmt, verfügen (erstellt von Pipeline 2 in Data Factory 1), markieren Sie Dataset4 als externes Dataset, da das Dataset von einer anderen Data Factory (DataFactory1, nicht DataFactory2) erstellt wird.  \n\n**DataFactory2**    \nPipeline 1: dataset4 > activity4 > dataset5\n\nWenn die Eigenschaft „external“ richtig festgelegt wurde, überprüfen Sie, ob die Eingabedaten an dem Speicherort existieren, der in der Definition des Eingabedatasets angegeben wurde.\n"
  - question: >
      Wie kann ein Slice zu einer anderen Zeit als Mitternacht ausgeführt werden, wenn der Slice täglich erstellt wird?
    answer: "Verwenden Sie die **offset** -Eigenschaft, um die Zeit anzugeben, zu der der Slice erstellt werden soll. Weitere Informationen zu dieser Eigenschaft finden Sie im Abschnitt [Dataset: Availability](data-factory-create-datasets.md#dataset-availability) . Hier ist ein kurzes Beispiel:\n\n```json\n\"availability\":\n{\n    \"frequency\": \"Day\",\n    \"interval\": 1,\n    \"offset\": \"06:00:00\"\n}\n```\nTägliche Slices starten anstatt zur Standardzeit (Mitternacht) um **6:00 Uhr** .     \n"
  - question: >
      Wie kann ich einen Slice erneut ausführen?
    answer: "Sie können einen Slice auf eine der folgenden Arten erneut ausführen:\n\n* Verwenden Sie die App „Überwachen und Verwalten“, um ein Aktivitätsfenster oder einen Slice erneut auszuführen. Anweisungen finden Sie unter [Wiederholen ausgewählter Aktivitätsfenster](data-factory-monitor-manage-app.md#perform-batch-actions) .   \n* Klicken Sie im Azure-Portal auf der Befehlsleiste für den Slice auf dem Blatt **DATENSLICE** auf **Ausführen**.\n* Führen Sie das Cmdlet **Set-AzDataFactorySliceStatus** aus, wobei der Status des Slice auf **Waiting** festgelegt ist.   \n\n    ```powershell\n    Set-AzDataFactorySliceStatus -Status Waiting -ResourceGroupName $ResourceGroup -DataFactoryName $df -TableName $table -StartDateTime \"02/26/2015 19:00:00\" -EndDateTime \"02/26/2015 20:00:00\"\n    ```\n  Unter [Set-AzDataFactorySliceStatus](/powershell/module/az.datafactory/set-Azdatafactoryslicestatus) finden Sie ausführliche Informationen zum Cmdlet.\n"
  - question: >
      Wie lange hat die Verarbeitung eines Slices gedauert?
    answer: "Verwenden Sie den Aktivitätsfenster-Explorer in der App „Überwachen und Verwalten“, um zu erfahren, wie lange es gedauert hat, einen Datenslice zu verarbeiten. Weitere Informationen finden Sie unter [Aktivitätsfenster-Explorer](data-factory-monitor-manage-app.md#activity-window-explorer) .\n\nSie können auch wie folgt im Azure-Portal vorgehen:  \n\n1. Klicken Sie auf der Kachel **Datasets** auf das Blatt **DATA FACTORY** für Ihre Data Factory.\n2. Klicken Sie auf dem Blatt **Datasets** auf das gewünschte Dataset.\n3. Wählen Sie auf dem Blatt **TABELLE** in der Liste **Zuletzt verwendete Slices** den gewünschten Slice aus.\n4. Klicken Sie auf dem Blatt **DATENSLICE** in der Liste **Aktivitätsausführungen** auf die Aktivitätsausführung.\n5. Klicken Sie auf der Kachel **Eigenschaften** auf das Blatt **DETAILS ZUR AKTIVITÄTSAUSFÜHRUNG**.\n6. Daraufhin sollte das Feld **Dauer** mit einem Wert angezeigt werden. Dieser Wert ist die Verarbeitungszeit des Slices.   \n"
  - question: >
      Wie wird ein ausgeführter Slice beendet?
    answer: >-
      Wenn Sie die Ausführung der Pipeline beenden müssen, können Sie das Cmdlet [Suspend-AzDataFactoryPipeline](/powershell/module/az.datafactory/suspend-azdatafactorypipeline) verwenden. Derzeit werden laufende Sliceausführungen bei Anhalten der Pipeline nicht beendet. Sobald die laufenden Ausführungen abgeschlossen sind, wird kein zusätzlicher Slice ausgewählt.


      Wenn Sie alle Ausführungen wirklich sofort beenden möchten, ist die einzige Möglichkeit das Löschen und erneute Erstellen der Pipeline. Wenn Sie die Pipeline löschen, müssen Sie keine Tabellen und verknüpften Dienste löschen, die von der Pipeline verwendet werden.
