---
title: Markupsprache f√ºr Sprachsynthese (Speech Synthesis Markup Language, SSML) ‚Äì Speech-Dienst
titleSuffix: Azure Cognitive Services
description: Verwenden der Markupsprache f√ºr Sprachsynthese zum Steuern der Aussprache und des Satzrhythmus in Text-zu-Sprache
services: cognitive-services
author: trevorbye
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 03/23/2020
ms.author: trbye
ms.custom: devx-track-js, devx-track-csharp
ms.openlocfilehash: 3ba2dad93778e9d4482fa00c854a73dbc616d290
ms.sourcegitcommit: 8bca2d622fdce67b07746a2fb5a40c0c644100c6
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 06/09/2021
ms.locfileid: "111750405"
---
# <a name="improve-synthesis-with-speech-synthesis-markup-language-ssml"></a>Verbessern der Synthese mit Markupsprache f√ºr Sprachsynthese (Speech Synthesis Markup Language, SSML)

Speech Synthesis Markup Language (SSML) ist eine XML-basierte Markupsprache, die Entwicklern erm√∂glicht, anzugeben, wie der Eingabetext mithilfe des Sprachsynthesediensts in synthetisierte Sprache konvertiert werden soll. Verglichen mit Nur-Text erm√∂glicht SSML Entwicklern, die Tonh√∂he, Aussprache, Sprechgeschwindigkeit, Lautst√§rke und mehr f√ºr die Ausgabe der Sprachsynthese zu optimieren. Die normale Interpunktion, z.B. das Pausieren nach einem Punkt, oder die Verwendung der korrekten Intonation, wenn ein Satz mit einem Fragezeichen endet, werden automatisch verarbeitet.

Die Speech-Dienstimplementierung von SSML basiert auf der [Markupsprache f√ºr Sprachsynthese, Version¬†1.0](https://www.w3.org/TR/speech-synthesis) des World Wide Web Consortiums.

> [!IMPORTANT]
> Chinesische, japanische und koreanische Zeichen z√§hlen bei der Abrechnung jeweils als zwei Zeichen. Weitere Informationen finden Sie unter [Preise](https://azure.microsoft.com/pricing/details/cognitive-services/speech-services/).

## <a name="neural-and-custom-voices"></a>Neuronale und benutzerdefinierte Stimmen

Verwenden Sie eine menschen√§hnliche neuronale Stimme, oder erstellen Sie Ihre eigene benutzerdefinierte Stimme speziell f√ºr Ihr Produkt oder Ihre Marke. Eine vollst√§ndige Liste der unterst√ºtzten Sprachen, Gebietsschemas und Stimmen finden Sie unter [Sprach- und Stimmunterst√ºtzung f√ºr den Speech-Dienst](language-support.md). Weitere Informationen zu neuronalen und benutzerdefinierten Stimmen finden Sie unter [Was ist Text-zu-Sprache?](text-to-speech.md).


> [!NOTE]
> Auf der Seite f√ºr [Sprachsynthese](https://azure.microsoft.com/services/cognitive-services/text-to-speech/#features) k√∂nnen Sie Stimmen Beispieltext in verschiedenen Stilen und Tonh√∂hen vorlesen h√∂ren.


## <a name="special-characters"></a>Sonderzeichen

Beachten Sie bei Verwendung von SSML, dass Sonderzeichen wie Anf√ºhrungszeichen, Apostrophe und Klammern mit Escapezeichen versehen werden m√ºssen. Weitere Informationen finden Sie unter [Extensible Markup Language (XML) 1.0: Anhang D](https://www.w3.org/TR/xml/#sec-entexpand).

## <a name="supported-ssml-elements"></a>Unterst√ºtzte SSML-Elemente

Jedes SSML-Dokument wird mit SSML-Elementen (oder Tags) erstellt. Diese Elemente werden zum Anpassen von Tonh√∂he, Satzrhythmus, Lautst√§rke und mehr verwendet. In den folgenden Abschnitten wird erl√§utert, wie die einzelnen Elemente verwendet werden und wann ein Element erforderlich oder optional ist.

> [!IMPORTANT]
> Vergessen Sie nicht die Eingabe von doppelten Anf√ºhrungszeichen um Attributwerte. Die Standards f√ºr wohlgeformtes, g√ºltiges XML erfordern es, dass Attributwerte in doppelten Anf√ºhrungszeichen stehen. So ist `<prosody volume="90">` ein wohlgeformtes, g√ºltiges Element, `<prosody volume=90>` aber nicht. SSML erkennt m√∂glicherweise keine Attributwerte, die nicht in Anf√ºhrungszeichen stehen.

## <a name="create-an-ssml-document"></a>Erstellen eines SSML-Dokuments

`speak` ist das Stammelement und bei allen SSML-Dokumenten **erforderlich**. Das `speak`-Element enth√§lt wichtige Informationen, z.B. die Version, Sprache und die Definition des Markupvokabulars.

**Syntax**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="string"></speak>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `version` | Gibt die Version der SSML-Spezifikation an, die zum Interpretieren des Dokumentmarkups verwendet wird. Die aktuelle Version ist 1.0. | Erforderlich |
| `xml:lang` | Gibt die Sprache des Stammdokuments an. Der Wert kann einen Kleinbuchstaben, einen Sprachcode aus zwei Buchstaben (z.¬†B. `en`) oder den Sprachcode und Land/Region aus Gro√übuchstaben (z.¬†B. `en-US`) enthalten. | Erforderlich |
| `xmlns` | Gibt den URI zu dem Dokument an, in dem das Markupvokabular (die Elementtypen und Attributnamen) des SSML-Dokuments definiert werden. Der aktuelle URI ist http://www.w3.org/2001/10/synthesis. | Erforderlich |

## <a name="choose-a-voice-for-text-to-speech"></a>Ausw√§hlen einer Stimme f√ºr Sprachsynthese

Das `voice`-Element ist erforderlich. Hiermit wird die Stimme angeben, f√ºr die Sprachsynthese verwendet wird.

**Syntax**

```xml
<voice name="string">
    This text will get converted into synthesized speech.
</voice>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `name` | Identifiziert die Stimme, die f√ºr die Ausgabe der Sprachsynthese verwendet wird. Eine vollst√§ndige Liste der unterst√ºtzten Stimmen finden Sie unter [Sprachunterst√ºtzung](language-support.md#text-to-speech). | Erforderlich |

**Beispiel**

> [!NOTE]
> In diesem Beispiel wird die Stimme `en-US-JennyNeural` verwendet. Eine vollst√§ndige Liste der unterst√ºtzten Stimmen finden Sie unter [Sprachunterst√ºtzung](language-support.md#text-to-speech).

```XML
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        This is the text that is spoken.
    </voice>
</speak>
```

## <a name="use-multiple-voices"></a>Verwenden mehrerer Stimmen

Innerhalb des `speak`-Elements k√∂nnen Sie mehrere Stimmen f√ºr die Ausgabe der Sprachsynthese angeben. Diese Stimmen k√∂nnen in verschiedenen Sprachen sein. Der Text muss bei jeder Stimme von einem `voice`-Element umschlossen werden.

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `name` | Identifiziert die Stimme, die f√ºr die Ausgabe der Sprachsynthese verwendet wird. Eine vollst√§ndige Liste der unterst√ºtzten Stimmen finden Sie unter [Sprachunterst√ºtzung](language-support.md#text-to-speech). | Erforderlich |

**Beispiel**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        Good morning!
    </voice>
    <voice name="en-US-GuyNeural">
        Good morning to you too Jenny!
    </voice>
</speak>
```

## <a name="adjust-speaking-styles"></a>Anpassen von Sprechweisen

Standardm√§√üig synthetisiert der Sprachsynthesedienst Text mit einer neutralen Sprechweise f√ºr neuronale Stimmen. Mithilfe des Elements `mstts:express-as` k√∂nnen Sie die Sprechweise anpassen, um verschiedene Emotionen wie Fr√∂hlichkeit, Mitgef√ºhl oder Gelassenheit auszudr√ºcken, oder die Stimme f√ºr verschiedene Szenarien wie Kundenservice, Nachrichtenpr√§sentation oder Sprach-Assistent optimieren. Dies ist ein optionales Element und f√ºr den Speech-Dienst eindeutig.

Anpassungen der Sprechweise werden derzeit f√ºr folgende neuronale Stimmen unterst√ºtzt:
* `en-US-AriaNeural`
* `en-US-JennyNeural`
* `en-US-GuyNeural`
* `pt-BR-FranciscaNeural`
* `zh-CN-XiaoxiaoNeural`
* `zh-CN-YunyangNeural`
* `zh-CN-YunyeNeural`
* `zh-CN-YunxiNeural`
* `zh-CN-XiaohanNeural`
* `zh-CN-XiaomoNeural`
* `zh-CN-XiaoxuanNeural`
* `zh-CN-XiaoruiNeural`

Die Intensit√§t der Sprechweise kann weiter ver√§ndert werden, damit sie besser zu Ihrem Anwendungsfall passt. Sie k√∂nnen mit `styledegree` eine kr√§ftigere oder sanftere Sprechweise angeben, um die Sprache ausdrucksst√§rker oder ged√§mpfter zu gestalten. Zurzeit werden Anpassungen in der Sprechweise der  neuronalen Stimmen f√ºr Chinesisch (Mandarin, vereinfacht) unterst√ºtzt.

Abgesehen von der Anpassung der Sprechweisen und ihrer Abstufungen k√∂nnen Sie auch den `role`-Parameter anpassen, damit die Stimme ein anderes Alter und Geschlecht imitiert. Beispielsweise kann eine m√§nnliche Stimme die Tonh√∂he erh√∂hen und die Intonation so √§ndern, dass eine weibliche Stimme imitiert wird, aber der Stimmname wird nicht ge√§ndert. Aktuell werden Rollenanpassungen bei folgenden neuronalen Stimmen f√ºr Chinesisch (Mandarin, vereinfacht) unterst√ºtzt:
* `zh-CN-XiaomoNeural`
* `zh-CN-XiaoxuanNeural`

Die voranstehenden √Ñnderungen werden auf Satzebene angewendet, und die Sprechweisen und Rollen variieren je nach Stimme. Wenn eine Sprechweise oder Rolle nicht unterst√ºtzt wird, gibt der Dienst Sprache in der neutralen Standardsprechweise zur√ºck. Die unterst√ºtzten Sprechweisen und Rollen f√ºr die jeweilige Stimme k√∂nnen mithilfe der [Stimmlisten-API](rest-text-to-speech.md#get-a-list-of-voices) oder √ºber die codefreie Plattform [Audioinhaltserstellung](https://aka.ms/audiocontentcreation) angezeigt werden.

**Syntax**

```xml
<mstts:express-as style="string"></mstts:express-as>
```
```xml
<mstts:express-as style="string" styledegree="value"></mstts:express-as>
```
```xml
<mstts:express-as role="string" style="string"></mstts:express-as>
```
> [!NOTE]
> Zurzeit unterst√ºtzt `styledegree` nur chinesische (Mandarin, vereinfacht) neuronale Stimmen. `role` unterst√ºtzt nur zh-CN-XiaomoNeural und zh-CN-XiaoxuanNeural.

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `style` | Gibt die Sprechweise an. Sprechweisen sind derzeit stimmenspezifisch. | Erforderlich, wenn die Sprechweise f√ºr eine neuronale Stimme angepasst wird. Bei Verwendung von `mstts:express-as` muss die Sprechweise angegeben werden. Bei Angabe eines ung√ºltigen Werts wird dieses Element ignoriert. |
| `styledegree` | Gibt die Intensit√§t der Sprechweise an. **Zul√§ssige Werte**: 0,01 bis 2 (einschlie√ülich). Der Standardwert ist 1, d.¬†h. die vordefinierte Intensit√§t f√ºr die Sprechweise. Die minimale Einheit ist 0,01, was zu einer leichten Tendenz zur Zielsprechweise f√ºhrt. Ein Wert von 2 f√ºhrt zu einer Verdoppelung der standardm√§√üigen Intensit√§t der Sprechweise.  | Optional (Zurzeit unterst√ºtzt `styledegree` nur chinesische (Mandarin, vereinfacht) neuronale Stimmen.)|
| `role` | Gibt die Sprechrolle an. Die Stimme imitiert ein anderes Alter und Geschlecht, aber der Sprachname wird nicht ge√§ndert.  | Optional (zurzeit unterst√ºtzt `role` nur zh-CN-XiaomoNeural und zh-CN-XiaoxuanNeural)|

Ermitteln Sie anhand dieser Tabelle, welche Sprechweisen f√ºr die einzelnen neuronalen Stimmen unterst√ºtzt werden.

| Sprache                   | Style                     | BESCHREIBUNG                                                 |
|-------------------------|---------------------------|-------------------------------------------------------------|
| `en-US-AriaNeural`      | `style="newscast-formal"` | Formaler, souver√§ner und verbindlicher Ton f√ºr die Mitteilung von Nachrichten |
|                         | `style="newscast-casual"` | Gewandter und ungezwungener Ton f√ºr die Mitteilung allgemeiner Nachrichten        |
|                         | `style="narration-professional"` | Professioneller und objektiver Ton f√ºr das Lesen von Inhalten        |
|                         | `style="customerservice"` | Freundlicher und hilfsbereiter Ton f√ºr den Kundensupport  |
|                         | `style="chat"`            | Lockerer und zwangloser Ton                         |
|                         | `style="cheerful"`        | Positiver und fr√∂hlicher Ton                         |
|                         | `style="empathetic"`      | Dr√ºckt ein Gef√ºhl von Anteilnahme und Verst√§ndnis aus               |
| `en-US-JennyNeural`     | `style="customerservice"` | Freundlicher und hilfsbereiter Ton f√ºr den Kundensupport  |
|                         | `style="chat"`            | Lockerer und zwangloser Ton                         |
|                         | `style="assistant"`       | Herzlicher und zwangloser Ton f√ºr digitale Assistenten    |
|                         | `style="newscast"`        | Gewandter und ungezwungener Ton f√ºr die Mitteilung allgemeiner Nachrichten   |
| `en-US-GuyNeural`       | `style="newscast"`        | Formeller und professioneller Ton f√ºr Nachrichten |
| `pt-BR-FranciscaNeural` | `style="calm"`            | K√ºhle, gesammelte und gelassene Haltung beim Sprechen Ton, Tonh√∂he und Intonation sind im Vergleich zu anderen Sprachtypen viel einheitlicher                                |
| `zh-CN-XiaoxiaoNeural`  | `style="newscast"`        | Formeller und professioneller Ton f√ºr Nachrichten |
|                         | `style="customerservice"` | Freundlicher und hilfsbereiter Ton f√ºr den Kundensupport  |
|                         | `style="assistant"`       | Herzlicher und zwangloser Ton f√ºr digitale Assistenten    |
|                         | `style="chat"`            | Lockerer und zwangloser Ton f√ºr Smalltalk           |
|                         | `style="calm"`            | K√ºhle, gesammelte und gelassene Haltung beim Sprechen Ton, Tonh√∂he und Intonation sind im Vergleich zu anderen Sprachtypen viel einheitlicher                                |
|                         | `style="cheerful"`        | Optimistischer und enthusiastischer Ton mit h√∂herer Tonh√∂he und stimmlicher Energie                         |
|                         | `style="sad"`             | Trauriger Ton mit h√∂herer Tonh√∂he, geringerer Intensit√§t und geringerer stimmlicher Energie H√§ufige Indikatoren f√ºr diese Emotion w√§ren Wimmern oder Weinen w√§hrend der Rede            |
|                         | `style="angry"`           | W√ºtender und ver√§rgerter Ton mit geringerer Tonh√∂he, h√∂herer Intensit√§t und h√∂herer stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er w√ºtend, unzufrieden und beleidigt ist.       |
|                         | `style="fearful"`         | √Ñngstlicher und nerv√∂ser Ton mit h√∂herer Tonh√∂he, h√∂herer stimmlicher Energie und h√∂herem Tempo Der Sprecher befindet sich in einem Zustand der Anspannung und Beunruhigung.                          |
|                         | `style="disgruntled"`     | Ver√§chtlicher und klagender Ton Eine Rede mit dieser Emotion zeugt von Unmut und Verachtung.              |
|                         | `style="serious"`         | Strenger und gebieterischer Ton Der Sprecher klingt oft steifer und viel weniger entspannt mit festem Rhythmus.          |
|                         | `style="affectionate"`    | Warmer und herzlicher Ton mit h√∂herer Tonh√∂he und stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er die Aufmerksamkeit der Zuh√∂rer auf sich zieht. Die ‚ÄûPers√∂nlichkeit‚Äú des Sprechers ist oft von liebenswerter Art.          |
|                         | `style="gentle"`          | Sanfter, h√∂flicher und angenehmer Ton mit geringerer Tonh√∂he und stimmlicher Energie         |
|                         | `style="lyrical"`         | Melodischer und gef√ºhlvoller Ton zum Ausdr√ºcken von Emotionen         |
| `zh-CN-YunyangNeural`   | `style="customerservice"` | Freundlicher und hilfsbereiter Ton f√ºr den Kundensupport  |
| `zh-CN-YunyeNeural`     | `style="calm"`            | K√ºhle, gesammelte und gelassene Haltung beim Sprechen Ton, Tonh√∂he und Intonation sind im Vergleich zu anderen Sprachtypen viel einheitlicher    |
|                         | `style="cheerful"`        | Optimistischer und enthusiastischer Ton mit h√∂herer Tonh√∂he und stimmlicher Energie                         |
|                         | `style="sad"`             | Trauriger Ton mit h√∂herer Tonh√∂he, geringerer Intensit√§t und geringerer stimmlicher Energie H√§ufige Indikatoren f√ºr diese Emotion w√§ren Wimmern oder Weinen w√§hrend der Rede            |
|                         | `style="angry"`           | W√ºtender und ver√§rgerter Ton mit geringerer Tonh√∂he, h√∂herer Intensit√§t und h√∂herer stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er w√ºtend, unzufrieden und beleidigt ist.       |
|                         | `style="fearful"`         | √Ñngstlicher und nerv√∂ser Ton mit h√∂herer Tonh√∂he, h√∂herer stimmlicher Energie und h√∂herem Tempo Der Sprecher befindet sich in einem Zustand der Anspannung und Beunruhigung.                          |
|                         | `style="disgruntled"`     | Ver√§chtlicher und klagender Ton Eine Rede mit dieser Emotion zeugt von Unmut und Verachtung.              |
|                         | `style="serious"`         | Strenger und gebieterischer Ton Der Sprecher klingt oft steifer und viel weniger entspannt mit festem Rhythmus.          |
|   `zh-CN-YunxiNeural`   | `style="assistant"`       | Herzlicher und zwangloser Ton f√ºr digitale Assistenten    |
|                         | `style="cheerful"`        | Optimistischer und enthusiastischer Ton mit h√∂herer Tonh√∂he und stimmlicher Energie                         |
|                         | `style="sad"`             | Trauriger Ton mit h√∂herer Tonh√∂he, geringerer Intensit√§t und geringerer stimmlicher Energie H√§ufige Indikatoren f√ºr diese Emotion w√§ren Wimmern oder Weinen w√§hrend der Rede            |
|                         | `style="angry"`           | W√ºtender und ver√§rgerter Ton mit geringerer Tonh√∂he, h√∂herer Intensit√§t und h√∂herer stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er w√ºtend, unzufrieden und beleidigt ist.       |
|                         | `style="fearful"`         | √Ñngstlicher und nerv√∂ser Ton mit h√∂herer Tonh√∂he, h√∂herer stimmlicher Energie und h√∂herem Tempo Der Sprecher befindet sich in einem Zustand der Anspannung und Beunruhigung.                          |
|                         | `style="disgruntled"`     | Ver√§chtlicher und klagender Ton Eine Rede mit dieser Emotion zeugt von Unmut und Verachtung.              |
|                         | `style="serious"`         | Strenger und gebieterischer Ton Der Sprecher klingt oft steifer und viel weniger entspannt mit festem Rhythmus.    |
|                         | `style="depressed"`       | Melancholischer und niedergeschlagener Ton mit geringerer Tonh√∂he und weniger Energie    |
|                         | `style="embarrassed"`     | Unsicherer und z√∂gerlicher Ton, wenn sich der Sprecher unwohl f√ºhlt   |
| `zh-CN-XiaohanNeural`   | `style="cheerful"`        | Optimistischer und enthusiastischer Ton mit h√∂herer Tonh√∂he und stimmlicher Energie                         |
|                         | `style="sad"`             | Trauriger Ton mit h√∂herer Tonh√∂he, geringerer Intensit√§t und geringerer stimmlicher Energie H√§ufige Indikatoren f√ºr diese Emotion w√§ren Wimmern oder Weinen w√§hrend der Rede            |
|                         | `style="angry"`           | W√ºtender und ver√§rgerter Ton mit geringerer Tonh√∂he, h√∂herer Intensit√§t und h√∂herer stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er w√ºtend, unzufrieden und beleidigt ist.       |
|                         | `style="fearful"`         | √Ñngstlicher und nerv√∂ser Ton mit h√∂herer Tonh√∂he, h√∂herer stimmlicher Energie und h√∂herem Tempo Der Sprecher befindet sich in einem Zustand der Anspannung und Beunruhigung.                          |
|                         | `style="disgruntled"`     | Ver√§chtlicher und klagender Ton Eine Rede mit dieser Emotion zeugt von Unmut und Verachtung.              |
|                         | `style="serious"`         | Strenger und gebieterischer Ton Der Sprecher klingt oft steifer und viel weniger entspannt mit festem Rhythmus.    |
|                         | `style="embarrassed"`     | Unsicherer und z√∂gerlicher Ton, wenn sich der Sprecher unwohl f√ºhlt   |
|                         | `style="affectionate"`    | Warmer und herzlicher Ton mit h√∂herer Tonh√∂he und stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er die Aufmerksamkeit der Zuh√∂rer auf sich zieht. Die ‚ÄûPers√∂nlichkeit‚Äú des Sprechers ist oft von liebenswerter Art.          |
|                         | `style="gentle"`          | Sanfter, h√∂flicher und angenehmer Ton mit geringerer Tonh√∂he und stimmlicher Energie         |
| `zh-CN-XiaomoNeural`    | `style="calm"`            | K√ºhle, gesammelte und gelassene Haltung beim Sprechen Ton, Tonh√∂he und Intonation sind im Vergleich zu anderen Sprachtypen viel einheitlicher                         |
|                         | `style="cheerful"`        | Optimistischer und enthusiastischer Ton mit h√∂herer Tonh√∂he und stimmlicher Energie                 |
|                         | `style="angry"`           | W√ºtender und ver√§rgerter Ton mit geringerer Tonh√∂he, h√∂herer Intensit√§t und h√∂herer stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er w√ºtend, unzufrieden und beleidigt ist.       |
|                         | `style="fearful"`         | √Ñngstlicher und nerv√∂ser Ton mit h√∂herer Tonh√∂he, h√∂herer stimmlicher Energie und h√∂herem Tempo Der Sprecher befindet sich in einem Zustand der Anspannung und Beunruhigung.                       |
|                         | `style="disgruntled"`     | Ver√§chtlicher und klagender Ton Eine Rede mit dieser Emotion zeugt von Unmut und Verachtung.         |
|                         | `style="serious"`         | Strenger und gebieterischer Ton Der Sprecher klingt oft steifer und viel weniger entspannt mit festem Rhythmus.  |
|                         | `style="depressed"`       | Melancholischer und niedergeschlagener Ton mit geringerer Tonh√∂he und weniger Energie    |
|                         | `style="gentle"`          | Sanfter, h√∂flicher und angenehmer Ton mit geringerer Tonh√∂he und stimmlicher Energie         |
| `zh-CN-XiaoxuanNeural`  | `style="calm"`            | K√ºhle, gesammelte und gelassene Haltung beim Sprechen Ton, Tonh√∂he und Intonation sind im Vergleich zu anderen Sprachtypen viel einheitlicher                         |
|                         | `style="cheerful"`        | Optimistischer und enthusiastischer Ton mit h√∂herer Tonh√∂he und stimmlicher Energie                              |
|                         | `style="angry"`           | W√ºtender und ver√§rgerter Ton mit geringerer Tonh√∂he, h√∂herer Intensit√§t und h√∂herer stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er w√ºtend, unzufrieden und beleidigt ist.       |
|                         | `style="fearful"`         | √Ñngstlicher und nerv√∂ser Ton mit h√∂herer Tonh√∂he, h√∂herer stimmlicher Energie und h√∂herem Tempo Der Sprecher befindet sich in einem Zustand der Anspannung und Beunruhigung.                       |
|                         | `style="disgruntled"`     | Ver√§chtlicher und klagender Ton Eine Rede mit dieser Emotion zeugt von Unmut und Verachtung.         |
|                         | `style="serious"`         | Strenger und gebieterischer Ton Der Sprecher klingt oft steifer und viel weniger entspannt mit festem Rhythmus.  |
|                         | `style="depressed"`       | Melancholischer und niedergeschlagener Ton mit geringerer Tonh√∂he und weniger Energie    |
|                         | `style="gentle"`          | Sanfter, h√∂flicher und angenehmer Ton mit geringerer Tonh√∂he und stimmlicher Energie         |
| `zh-CN-XiaoruiNeural`   | `style="sad"`             | Trauriger Ton mit h√∂herer Tonh√∂he, geringerer Intensit√§t und geringerer stimmlicher Energie H√§ufige Indikatoren f√ºr diese Emotion w√§ren Wimmern oder Weinen w√§hrend der Rede         |
|                         | `style="angry"`           | W√ºtender und ver√§rgerter Ton mit geringerer Tonh√∂he, h√∂herer Intensit√§t und h√∂herer stimmlicher Energie Der Sprecher ist in einem Zustand, in dem er w√ºtend, unzufrieden und beleidigt ist.       |
|                         | `style="fearful"`         | √Ñngstlicher und nerv√∂ser Ton mit h√∂herer Tonh√∂he, h√∂herer stimmlicher Energie und h√∂herem Tempo Der Sprecher befindet sich in einem Zustand der Anspannung und Beunruhigung.                       |

Verwenden Sie diese Tabelle, um die unterst√ºtzten Rollen und deren Definitionen zu √ºberpr√ºfen.

|Role                     | BESCHREIBUNG                |
|-------------------------|----------------------------|
|`role="Girl"`            | Die Stimme imitiert ein M√§dchen. |
|`role="Boy"`             | Die Stimme imitiert einen Jungen. |
|`role="YoungAdultFemale"`| Die Stimme imitiert eine junge erwachsene Frau.|
|`role="YoungAdultMale"`  | Die Stimme imitiert einen jungen erwachsenen Mann.|
|`role="OlderAdultFemale"`| Die Stimme imitiert eine √§ltere erwachsene Frau.|
|`role="OlderAdultMale"`  | Die Stimme imitiert einen √§lteren erwachsenen Mann.|
|`role="SeniorFemale"`    | Die Stimme imitiert eine reife Frau.|
|`role="SeniorMale"`      | Die Stimme imitiert einen reifen Mann.|


**Beispiel**

Dieser SSML-Codeausschnitt veranschaulicht, wie die Sprechweise mithilfe des `<mstts:express-as>`-Elements in `cheerful` (fr√∂hlich) ge√§ndert wird.

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
       xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="en-US">
    <voice name="en-US-AriaNeural">
        <mstts:express-as style="cheerful">
            That'd be just amazing!
        </mstts:express-as>
    </voice>
</speak>
```

Dieser SSML-Codeausschnitt veranschaulicht, wie das Attribut `styledegree` verwendet wird, um die Intensit√§t der Sprechweise f√ºr XiaoxiaoNeural zu √§ndern.
```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
       xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="zh-CN">
    <voice name="zh-CN-XiaoxiaoNeural">
        <mstts:express-as style="sad" styledegree="2">
            Âø´Ëµ∞ÂêßÔºåË∑Ø‰∏ä‰∏ÄÂÆöË¶ÅÊ≥®ÊÑèÂÆâÂÖ®ÔºåÊó©ÂéªÊó©Âõû„ÄÇ
        </mstts:express-as>
    </voice>
</speak>
```

Dieser SSML-Codeausschnitt veranschaulicht, wie das Attribut `role` verwendet wird, um die Rolle f√ºr XiaomoNeural zu √§ndern.
```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
       xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="zh-CN">
    <voice name="zh-CN-XiaomoNeural">
        Â•≥ÂÑøÁúãËßÅÁà∂‰∫≤Ëµ∞‰∫ÜËøõÊù•ÔºåÈóÆÈÅìÔºö
        <mstts:express-as role="YoungAdultFemale" style="calm">
            ‚ÄúÊÇ®Êù•ÁöÑÊå∫Âø´ÁöÑÔºåÊÄé‰πàËøáÊù•ÁöÑÔºü‚Äù
        </mstts:express-as>
        Áà∂‰∫≤Êîæ‰∏ãÊâãÊèêÂåÖÔºåËØ¥Ôºö
        <mstts:express-as role="OlderAdultMale" style="calm">
            ‚ÄúÂàöÊâìËΩ¶ËøáÊù•ÁöÑÔºåË∑Ø‰∏äËøòÊå∫È°∫ÁïÖ„ÄÇ‚Äù
        </mstts:express-as>
    </voice>
</speak>
```

## <a name="adjust-speaking-languages"></a>Anpassen der gesprochenen Sprachen

Sie k√∂nnen die gesprochenen Sprachen f√ºr neuronale Stimmen anpassen.
Erm√∂glichen Sie es einer Stimme, mithilfe des `<lang xml:lang>`-Elements verschiedene Sprachen (z.¬†B. Englisch, Spanisch und Chinesisch) fl√ºssig zu sprechen. Dies ist ein optionales Element und f√ºr den Speech-Dienst eindeutig. Ohne dieses Element spricht die Stimme ihre prim√§re Sprache.
Anpassungen der gesprochenen Sprache werden derzeit bei den folgenden neuronalen Stimmen unterst√ºtzt: `en-US-JennyMultilingualNeural`. Die obigen √Ñnderungen werden auf der Satz- und Wortebene vorgenommen. Wenn eine Sprache nicht unterst√ºtzt wird, gibt der Dienst keinen Audiodatenstrom zur√ºck.

> [!NOTE]
> Derzeit ist das `<lang xml:lang>`-Element nicht mit dem `prosody`- und `break`-Element kompatibel. Sie k√∂nnen in diesem Element weder Pause noch Satzrhytmus wie Tonh√∂he, Kontur, Geschwindigkeit, Dauer, Lautst√§rke anpassen.

**Syntax**

```xml
<lang xml:lang="string"></lang>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `lang` | Gibt die gesprochenen Sprachen an. Derzeit ist das Sprechen verschiedener Sprachen von der Stimme abh√§ngig. | Erforderlich, wenn die gesprochene Sprache f√ºr eine neuronale Stimme angepasst wird. Bei Verwendung von `lang xml:lang` muss das Gebietsschema angegeben werden. |

Ermitteln Sie anhand dieser Tabelle, welche gesprochenen Sprachen f√ºr die einzelnen neuronalen Stimmen unterst√ºtzt werden.

| Sprache                            | Gebietsschemasprache           | Beschreibung                                                 |
|----------------------------------|---------------------------|-------------------------------------------------------------|
| `en-US-JennyMultilingualNeural`  | `lang="en-US"`            | Sprechen f√ºr das Gebietsschema en-US, dem prim√§ren Gebietsschema dieser Stimme |
|                                  | `lang="en-CA"`            | Sprechen der Gebietsschemasprache en-CA                                  |
|                                  | `lang="en-AU"`            | Sprechen der Gebietsschemasprache en-AU                                  |
|                                  | `lang="en-GB"`            | Sprechen der Gebietsschemasprache en-GB                                  |
|                                  | `lang="de-DE"`            | Sprechen der Gebietsschemasprache de-DE                                  |
|                                  | `lang="fr-FR"`            | Sprechen der Gebietsschemasprache fr-FR                                  |
|                                  | `lang="fr-CA"`            | Sprechen der Gebietsschemasprache fr-CA                                  |
|                                  | `lang="es-ES"`            | Sprechen der Gebietsschemasprache es-ES                                  |
|                                  | `lang="es-MX"`            | Sprechen der Gebietsschemasprache es-MX                                  |
|                                  | `lang="zh-CN"`            | Sprechen der Gebietsschemasprache zh-CN                                  |
|                                  | `lang="ko-KR"`            | Sprechen der Gebietsschemasprache ko-KR                                  |
|                                  | `lang="ja-JP"`            | Sprechen der Gebietsschemasprache ja-JP                                  |
|                                  | `lang="it-IT"`            | Sprechen der Gebietsschemasprache it-IT                                  |
|                                  | `lang="pt-BR"`            | Sprechen der Gebietsschemasprache pt-BR                                  |

**Beispiel**

Dieser SSML-Codeausschnitt zeigt, wie sie `<lang xml:lang>` verwenden, um die gesprochenen Sprachen in `en-US`, `es-MX` und `de-DE` zu √§ndern.

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
       xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="en-US">
    <voice name="en-US-JennyMultilingualNeural">
        I am looking forward to the exciting things.
        <lang xml:lang="es-MX">
            Estoy deseando que lleguen las cosas emocionantes.
        </lang>
        <lang xml:lang="de-DE">
            Ich freue mich auf die spannenden Dinge.
        </lang>
    </voice>
</speak>
```

## <a name="add-or-remove-a-breakpause"></a>Hinzuf√ºgen oder Entfernen einer Unterbrechung/Pause

Verwenden Sie das `break`-Element zum Einf√ºgen von Pausen (oder Unterbrechungen) zwischen W√∂rtern oder um Pausen zu verhindern, die vom Sprachsynthesedienst automatisch hinzugef√ºgt werden.

> [!NOTE]
> Mithilfe dieses Elements k√∂nnen Sie das Standardverhalten von TTS (Text-To-Speech, Text-zu-Sprache, Sprachsynthese) bei einem Wort oder Ausdruck au√üer Kraft setzen, wenn die synthetisierte Sprache daf√ºr unnat√ºrlich klingt. Legen Sie `strength` auf `none` fest, um eine Unterbrechung des Satzrhythmus zu verhindern. Dieses Element wird automatisch vom Text-zu-Sprache-Dienst eingef√ºgt.

**Syntax**

```xml
<break strength="string" />
<break time="string" />
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `strength` | Gibt die relative Dauer einer Pause mit einem der folgenden Werte an:<ul><li>none</li><li>x-weak</li><li>weak</li><li>medium (Standard)</li><li>strong</li><li>x-strong</li></ul> | Optional |
| `time` | Gibt die absolute Dauer einer Pause in Sekunden oder Millisekunden an. Dieser Werte sollte weniger als 5000¬†ms betragen. Beispiele f√ºr g√ºltige Werte sind `2s` und `500ms` | Optional |

| Strength                      | BESCHREIBUNG |
|-------------------------------|-------------|
| ‚ÄûNone‚Äú, oder wenn kein Wert angegeben | 0 ms        |
| x-weak                        | 250 ms      |
| weak                          | 500 ms      |
| mittel                        | 750 ms      |
| strong                        | 1\.000 ms     |
| x-strong                      | 1\.250 ms     |

**Beispiel**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-AriaNeural">
        Welcome to Microsoft Cognitive Services <break time="100ms" /> Text-to-Speech API.
    </voice>
</speak>
```
## <a name="add-silence"></a>Hinzuf√ºgen von Stille

Verwenden Sie das `mstts:silence`-Element, um Pausen vor oder nach Text oder zwischen den 2¬†benachbarten S√§tzen einzuf√ºgen.

> [!NOTE]
>Der Unterschied zwischen `mstts:silence` und `break` besteht darin, dass `break` an einer beliebigen Stelle im Text hinzugef√ºgt werden kann. Stille jedoch funktioniert nur am Anfang oder Ende von Eingabetext oder an der Grenze zwischen 2¬†benachbarten S√§tzen.


**Syntax**

```xml
<mstts:silence  type="string"  value="string"/>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `type` | Gibt die Position zum Hinzuf√ºgen von Stille an: <ul><li>`Leading` ‚Äì am Anfang von Text </li><li>`Tailing` ‚Äì am Ende von Text </li><li>`Sentenceboundary` ‚Äì zwischen benachbarten S√§tzen </li></ul> | Erforderlich |
| `Value` | Gibt die absolute Dauer einer Pause in Sekunden oder Millisekunden an. Dieser Werte sollte weniger als 5000¬†ms betragen. Beispiele f√ºr g√ºltige Werte sind `2s` und `500ms` | Erforderlich |

**Beispiel** In diesem Beispiel wird `mtts:silence` verwendet, um 200¬†ms Stille zwischen zwei S√§tzen hinzuzuf√ºgen.
```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
<voice name="en-US-AriaNeural">
<mstts:silence  type="Sentenceboundary" value="200ms"/>
If we‚Äôre home schooling, the best we can do is roll with what each day brings and try to have fun along the way.
A good place to start is by trying out the slew of educational apps that are helping children stay happy and smash their schooling at the same time.
</voice>
</speak>
```

## <a name="specify-paragraphs-and-sentences"></a>Angeben von Abs√§tzen und S√§tzen

`p`- und `s`-Elemente werden verwendet, um Abschnitte bzw. S√§tze zu bezeichnen. Wenn diese Elemente fehlen, ermittelt der Sprachsynthesedienst automatisch die Struktur des SSML-Dokuments.

Das `p`-Element kann Text und die folgenden Elemente enthalten: `audio`, `break`, `phoneme`, `prosody`, `say-as`, `sub`, `mstts:express-as` und `s`.

Das `s`-Element kann Text und die folgenden Elemente enthalten: `audio`, `break`, `phoneme`, `prosody`, `say-as`, `mstts:express-as` und `sub`.

**Syntax**

```XML
<p></p>
<s></s>
```

**Beispiel**

```XML
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <p>
            <s>Introducing the sentence element.</s>
            <s>Used to mark individual sentences.</s>
        </p>
        <p>
            Another simple paragraph.
            Sentence structure in this paragraph is not explicitly marked.
        </p>
    </voice>
</speak>
```

## <a name="use-phonemes-to-improve-pronunciation"></a>Verwenden von Phonemen zur Verbesserung der Aussprache

Das `ph`-Element wird f√ºr die phonetische Aussprache in SSML-Dokumenten verwendet. Das `ph`-Element kann nur Text und keine anderen Elemente enthalten. Geben Sie immer lesbare Sprache als ein Fallback an.

Phonetische Alphabete bestehen aus Phonen (Lauten), die sich aus Buchstaben, Zahlen oder Zeichen (manchmal in Kombination) zusammensetzen. Jedes Phon beschreibt einen eindeutigen Sprachklang. Dies steht im Gegensatz zum lateinischen Alphabet, in dem jeder Buchstabe mehrere gesprochene Kl√§nge darstellen kann. √úberlegen Sie die unterschiedliche Aussprache des Buchstabens ‚ÄûC‚Äú in den St√§dtenamen ‚ÄûCoburg‚Äú und ‚ÄûCelle‚Äú oder die unterschiedliche Aussprache der Buchstabenkombination ‚Äûch‚Äú in den W√∂rtern ‚Äûich‚Äú und ‚Äûach‚Äú.

> [!NOTE]
> Das ‚ÄûPhoneme‚Äú-Tag wird zurzeit f√ºr diese 5¬†Stimmen (et-EE-AnuNeural, ga-IE-OrlaNeural, lt-LT-OnaNeural, lv-LV-EveritaNeural und mt-MT-GarceNeural) nicht unterst√ºtzt.

**Syntax**

```XML
<phoneme alphabet="string" ph="string"></phoneme>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `alphabet` | Gibt das phonetische Alphabet an, das verwendet werden soll, wenn die Aussprache der Zeichenfolge im `ph`-Attribut synthetisiert wird. Die Zeichenfolge, die das Alphabet angibt, muss in Kleinbuchstaben angegeben werden. Nachstehend sind die Alphabete aufgef√ºhrt, die Sie angeben k√∂nnen.<ul><li>`ipa` &ndash; <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet" target="_blank">Internationales phonetisches Alphabet </a></li><li>`sapi` &ndash; [Phonetisches Alphabet des Speech-Diensts](speech-ssml-phonetic-sets.md)</li><li>`ups` &ndash;<a href="https://documentation.help/Microsoft-Speech-Platform-SDK-11/17509a49-cae7-41f5-b61d-07beaae872ea.htm" target="_blank"> Universal Phone Set</a></li></ul><br>Das Alphabet gilt nur f√ºr das `phoneme` im Element. | Optional |
| `ph` | Eine Zeichenfolge mit Phonen, die die Aussprache des Worts im `phoneme`-Element angeben. Wenn die angegebene Zeichenfolge nicht erkannte Phone enth√§lt, weist der Sprachsynthesedienst das gesamte SSML-Dokument zur√ºck und erzeugt keine der im Dokument angegebenen Sprachausgaben. | Erforderlich, wenn Phoneme verwendet werden. |

**Beispiele**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <phoneme alphabet="ipa" ph="t…ôÀàme…™to ä"> tomato </phoneme>
    </voice>
</speak>
```

```xml
<speak version="1.0" xmlns="https://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <phoneme alphabet="sapi" ph="iy eh n y uw eh s"> en-US </phoneme>
    </voice>
</speak>
```

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <s>His name is Mike <phoneme alphabet="ups" ph="JH AU"> Zhou </phoneme></s>
    </voice>
</speak>
```

## <a name="use-custom-lexicon-to-improve-pronunciation"></a>Verwenden eines benutzerdefinierten Lexikons zum Verbessern der Aussprache

Manchmal kann ein Wort vom Sprachsynthesedienst nicht korrekt ausgesprochen werden. Beispielsweise der Name eines Unternehmens, ein medizinischer Begriff oder ein Emoji. Mit den Tags `phoneme` und `sub` k√∂nnen Entwickler die Aussprache einzelner Entit√§ten in SSML definieren. Wenn Sie dagegen die Aussprache mehrerer Entit√§ten definieren m√∂chten, k√∂nnen Sie mithilfe des Tags `lexicon` ein benutzerdefiniertes Lexikon erstellen.

> [!NOTE]
> F√ºr das benutzerdefinierte Lexikon wird derzeit die UTF-8-Codierung unterst√ºtzt.

> [!NOTE]
> Das benutzerdefinierte Lexikon wird zurzeit f√ºr diese 5¬†Stimmen (et-EE-AnuNeural, ga-IE-OrlaNeural, lt-LT-OnaNeural, lv-LV-EveritaNeural und mt-MT-GarceNeural) nicht unterst√ºtzt.


**Syntax**

```XML
<lexicon uri="string"/>
```

**Attribute**

| attribute | BESCHREIBUNG                               | Erforderlich/optional |
|-----------|-------------------------------------------|---------------------|
| `uri`     | Die Adresse des externen PLS-Dokuments. | Erforderlich.           |

**Verwendung**

Wenn Sie die Aussprache mehrerer Entit√§ten definieren m√∂chten, k√∂nnen Sie ein benutzerdefiniertes Lexikon erstellen. Dieses wird als XML- oder als PLS-Datei gespeichert. Das folgende Beispiel zeigt eine XML-Datei:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<lexicon version="1.0"
      xmlns="http://www.w3.org/2005/01/pronunciation-lexicon"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon
        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd"
      alphabet="ipa" xml:lang="en-US">
  <lexeme>
    <grapheme>BTW</grapheme>
    <alias>By the way</alias>
  </lexeme>
  <lexeme>
    <grapheme> Benigni </grapheme>
    <phoneme> b…õÀàniÀênji</phoneme>
  </lexeme>
  <lexeme>
    <grapheme>üòÄ</grapheme>
    <alias>test emoji</alias>
  </lexeme>
</lexicon>
```

Das `lexicon`-Element enth√§lt mindestens ein `lexeme`-Element. Jedes `lexeme`-Element enth√§lt mindestens ein `grapheme`-Element und mindestens eines der Elemente `grapheme`, `alias` und `phoneme`. Das `grapheme`-Element enth√§lt Text, der die <a href="https://www.w3.org/TR/pronunciation-lexicon/#term-Orthography" target="_blank">Orthografie</a> beschreibt. Mithilfe der `alias`-Elemente wird die Aussprache eines Akronyms oder eines abgek√ºrzten Begriffs angegeben. Das `phoneme`-Element stellt Text bereit, der die Aussprache von `lexeme` beschreibt. Wenn die Elemente `alias` und `phoneme` mit demselben `grapheme`-Element bereitgestellt werden, weist `alias` eine h√∂here Priorit√§t auf.

Das Lexikon enth√§lt das erforderliche `xml:lang`-Attribut, um anzugeben, auf welches Gebietsschema es angewendet werden soll. Ein benutzerdefiniertes Lexikon ist standardm√§√üig auf ein einzelnes Gebietsschema beschr√§nkt, sodass die Anwendung auf ein anderes Gebietsschema nicht funktioniert.

Wichtig: Die Aussprache eines Ausdrucks kann mit dem benutzerdefinierten Lexikon nicht direkt festgelegt werden. Wenn Sie die Aussprache f√ºr ein Akronym oder einen abgek√ºrzten Begriff festlegen m√∂chten, m√ºssen Sie zuerst einen Alias (`alias`) angeben und anschlie√üend das Phonem (`phoneme`) diesem Alias (`alias`) zuordnen. Beispiel:

```xml
  <lexeme>
    <grapheme>Scotland MV</grapheme>
    <alias>ScotlandMV</alias>
  </lexeme>
  <lexeme>
    <grapheme>ScotlandMV</grapheme>
    <phoneme>Ààsk…ítl…ônd.ÀàmiÀêdi…ôm.we…™v</phoneme>
  </lexeme>
```

Sie k√∂nnen auch direkt ihren erwarteten `alias` f√ºr das Akronym oder einen abgek√ºrzten Begriff angeben. Beispiel:
```xml
  <lexeme>
    <grapheme>Scotland MV</grapheme>
    <alias>Scotland Media Wave</alias>
  </lexeme>
```

> [!IMPORTANT]
> Das Element `phoneme` darf bei Verwendung des IPA keine Leerzeichen enthalten.

Weitere Informationen zu benutzerdefinierten Lexikondateien finden Sie unter [Spezifikation f√ºr Aussprachelexika (Pronunciation Lexicon Specification, PLS), Version¬†1.0](https://www.w3.org/TR/pronunciation-lexicon/).

Ver√∂ffentlichen Sie als N√§chstes Ihre benutzerdefinierte Lexikondatei. Diese Datei kann zwar an einem beliebigen Ort gespeichert werden, es empfiehlt sich jedoch, [Azure Blob Storage](../../storage/blobs/storage-quickstart-blobs-portal.md) zu verwenden.

Nachdem Sie Ihr benutzerdefiniertes Lexikon ver√∂ffentlicht haben, k√∂nnen Sie von SSML aus darauf verweisen.

> [!NOTE]
> Das Element `lexicon` muss sich innerhalb des Elements `voice` befinden.

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis"
          xmlns:mstts="http://www.w3.org/2001/mstts"
          xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <lexicon uri="http://www.example.com/customlexicon.xml"/>
        BTW, we will be there probably at 8:00 tomorrow morning.
        Could you help leave a message to Robert Benigni for me?
    </voice>
</speak>
```

Wenn Sie dieses benutzerdefinierte Lexikon verwenden, wird ‚ÄûBTW‚Äú als ‚ÄûBy the way‚Äú gelesen. ‚ÄûBenigni‚Äú wird gem√§√ü der IPA-Angabe als ‚Äûb…õÀàniÀênji‚Äú gelesen.

**Einschr√§nkungen**
- Dateigr√∂√üe: Die maximale Gr√∂√üe benutzerdefinierter Lexikondateien betr√§gt 100¬†KB. Wenn diese Gr√∂√üe √ºberschritten wird, treten bei Syntheseanforderungen Fehler auf.
- Lexikoncacheaktualisierung: Das benutzerdefinierte Lexikon wird beim ersten Laden mit dem URI als Schl√ºssel im TTS-Dienst zwischengespeichert. Ein Lexikon mit demselben URI wird innerhalb von 15¬†Minuten nicht neu geladen, sodass √Ñnderungen am benutzerdefinierten Lexikon nach maximal 15¬†Minuten in Kraft treten.

**Phonetische S√§tze des Speech-Diensts**

Im obigen Beispiel wird das als IPA-Phonemsatz bezeichnete internationale phonetische Alphabet verwendet. Wir empfehlen Entwicklern, das IPA zu verwenden, da es der internationale Standard ist. Einige IPA-Zeichen k√∂nnen in Unicode als zusammengesetzte und als aufgel√∂ste Version dargestellt werden. Im benutzerdefinierten Lexikon wird nur die aufgel√∂ste Unicodedarstellung unterst√ºtzt.

Da das IPA nicht leicht zu merken ist, definiert der Speech-Dienst einen phonetischen Satz f√ºr sieben Sprachen (`en-US`, `fr-FR`, `de-DE`, `es-ES`, `ja-JP`, `zh-CN` und `zh-TW`).

`x-microsoft-sapi` kann wie unten gezeigt als Wert des Attributs `alphabet` mit benutzerdefinierten Lexika verwendet werden:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<lexicon version="1.0"
      xmlns="http://www.w3.org/2005/01/pronunciation-lexicon"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="http://www.w3.org/2005/01/pronunciation-lexicon
        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd"
      alphabet="x-microsoft-sapi" xml:lang="en-US">
  <lexeme>
    <grapheme>BTW</grapheme>
    <alias> By the way </alias>
  </lexeme>
  <lexeme>
    <grapheme> Benigni </grapheme>
    <phoneme> b eh 1 - n iy - n y iy </phoneme>
  </lexeme>
</lexicon>
```

Ausf√ºhrliche Informationen zum phonetischen Alphabet des Speech-Diensts finden Sie unter [Phonetische S√§tze des Speech-Diensts](speech-ssml-phonetic-sets.md).

## <a name="adjust-prosody"></a>Anpassen des Satzrhythmus

Das `prosody`-Element wird verwendet, um √Ñnderungen an Tonh√∂he, Kontur, Bereich, Geschwindigkeit, Dauer und Lautst√§rke f√ºr die Ausgabe der Sprachsynthese anzugeben. Das `prosody`-Element kann Text und die folgenden Elemente enthalten: `audio`, `break`, `p`, `phoneme`, `prosody`, `say-as`, `sub` und `s`.

Weil Attributwerte f√ºr den Satzrhythmus √ºber einen breiten Bereich variieren k√∂nnen, interpretiert die Spracherkennung die zugewiesenen Werte als einen Vorschlag dazu, wie die tats√§chlichen Satzrhythmuswerte f√ºr die ausgew√§hlte Stimme lauten sollten. Der Sprachsynthesedienst beschr√§nkt oder ersetzt nicht unterst√ºtzte Werte. Beispiele f√ºr nicht unterst√ºtzte Werte sind eine Tonh√∂he von 1 MHz oder eine Lautst√§rke von 120.

**Syntax**

```XML
<prosody pitch="value" contour="value" range="value" rate="value" duration="value" volume="value"></prosody>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `pitch` | Gibt die Basistonh√∂he f√ºr den Text an. Sie k√∂nnen die Tonh√∂he ausdr√ºcken als:<ul><li>Ein absoluter Wert, der ausgedr√ºckt wird als eine Zahl, hinter der‚ÄûHz‚Äú (Hertz) steht. Beispiel: `<prosody pitch="600Hz">some text</prosody>`.</li><li>Ein relativer Wert, der ausgedr√ºckt wird als eine Zahl, vor der ‚Äû+‚Äú oder ‚Äû‚Äì‚Äú und hinter der ‚ÄûHz‚Äú oder ‚Äûst‚Äústeht, das einen Betrag zur √Ñnderung der Tonh√∂he angibt. Beispiel: `<prosody pitch="+80Hz">some text</prosody>` oder `<prosody pitch="-2st">some text</prosody>`. Das ‚Äûst‚Äú gibt an, dass die √Ñnderungseinheit ein Halbton ist, bei dem es sich um die H√§lfte eines Tons (ein halber Schritt) auf der diatonischen Standardtonleiter handelt.</li><li>Einen konstanten Wert:<ul><li>x-low</li><li>niedrig</li><li>mittel</li><li>high</li><li>x-high</li><li>default</li></ul></li></ul> | Optional |
| `contour` |Die Kontur unterst√ºtzt jetzt sowohl neuronale als auch Standardstimmen. Die Kontur stellt √Ñnderungen der Tonh√∂he dar. Diese √Ñnderungen werden als ein Array von Zielen an den angegebenen Zeitpositionen in der Sprachausgabe dargestellt. Jedes Ziel wird durch Gruppen von Parameterpaaren definiert. Beispiel: <br/><br/>`<prosody contour="(0%,+20Hz) (10%,-2st) (40%,+10Hz)">`<br/><br/>Der erste Wert in jeder Gruppe von Parametern gibt den Ort der Tonh√∂hen√§nderung als Prozentsatz der Textdauer an. Der zweite Wert gibt den Betrag an, um den die Tonh√∂he erh√∂ht oder verringert werden soll. Dazu wird ein relativer Wert oder ein Aufz√§hlungswert f√ºr die Tonh√∂he verwendet (siehe `pitch`). | Optional |
| `range` | Ein Wert, der den Tonh√∂henbereich f√ºr den Text darstellt. Sie k√∂nnen `range` mit denselben absoluten Werten, relativen Werten oder Aufz√§hlungswerten ausdr√ºcken, mit denen `pitch` beschrieben wird. | Optional |
| `rate` | Gibt die Sprechgeschwindigkeit f√ºr den Text an. Sie k√∂nnen `rate` ausdr√ºcken als:<ul><li>Ein relativer Wert, der ausgedr√ºckt wird als eine Zahl, die als Multiplikator des Standards fungiert. So f√ºhrt beispielsweise der Wert *1* zu keiner √Ñnderung der Geschwindigkeit. Der Wert *0,5* f√ºhrt zu einer Halbierung der Geschwindigkeit. Der Wert *3* f√ºhrt zu einer Verdreifachung der Geschwindigkeit.</li><li>Einen konstanten Wert:<ul><li>x-slow</li><li>langsam</li><li>mittel</li><li>fast</li><li>x-fast</li><li>default</li></ul></li></ul> | Optional |
| `duration` | Die Zeitspanne in Sekunden oder Millisekunden, die vergehen sollte, w√§hrend der Sprachsynthesedienst den Text liest. Beispiel: *2s* oder *1800ms*. Die Dauer unterst√ºtzt nur Standardstimmen.| Optional |
| `volume` | Gibt die Lautst√§rke der Sprechstimme an. Sie k√∂nnen die Lautst√§rke ausdr√ºcken als:<ul><li>Ein absoluter Wert, der ausgedr√ºckt wird als eine Zahl im Bereich von 0,0 bis 100,0 ‚Äì von *am leisesten* bis zu *am lautesten*. Beispiel: ‚Äû75‚Äú. Der Standardwert ist ‚Äû100,0‚Äú.</li><li>Ein relativer Wert, der ausgedr√ºckt wird als eine Zahl, vor der ein ‚Äû+‚Äú oder ‚Äû‚Äì‚Äú steht und die einen Betrag zum √Ñndern der Lautst√§rke angibt. Beispiel: ‚Äû+10‚Äú oder ‚Äû-5,5‚Äú.</li><li>Einen konstanten Wert:<ul><li>silent</li><li>x-soft</li><li>soft</li><li>mittel</li><li>loud</li><li>x-loud</li><li>default</li></ul></li></ul> | Optional |

### <a name="change-speaking-rate"></a>√Ñndern der Sprechgeschwindigkeit

Die Sprechgeschwindigkeit kann auf neuronale Stimmen und Standardstimmen auf Wort- oder Satzebene angewendet werden.

**Beispiel**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-GuyNeural">
        <prosody rate="+30.00%">
            Welcome to Microsoft Cognitive Services Text-to-Speech API.
        </prosody>
    </voice>
</speak>
```

### <a name="change-volume"></a>√Ñndern der Lautst√§rke

√Ñnderungen der Lautst√§rke k√∂nnen auf Standardstimmen auf Wort- oder Satzebene angewendet werden. √Ñnderungen der Lautst√§rke k√∂nnen dagegen nur auf neuronale Stimmen auf Satzebene angewendet werden.

**Beispiel**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <prosody volume="+20.00%">
            Welcome to Microsoft Cognitive Services Text-to-Speech API.
        </prosody>
    </voice>
</speak>
```

### <a name="change-pitch"></a>√Ñndern der Tonh√∂he

√Ñnderungen der Tonh√∂he k√∂nnen auf Standardstimmen auf Wort- oder Satzebene angewendet werden. √Ñnderungen der Tonh√∂he k√∂nnen dagegen nur auf neuronale Stimmen auf Satzebene angewendet werden.

**Beispiel**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-AriaNeural">
        Welcome to <prosody pitch="high">Microsoft Cognitive Services Text-to-Speech API.</prosody>
    </voice>
</speak>
```

### <a name="change-pitch-contour"></a>√Ñndern der Tonh√∂henkontur

> [!IMPORTANT]
> √Ñnderungen der Tonh√∂henkontur werden jetzt f√ºr neuronale Stimmen unterst√ºtzt.

**Beispiel**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-AriaNeural">
        <prosody contour="(60%,-60%) (100%,+80%)" >
            Were you the only person in the room?
        </prosody>
    </voice>
</speak>
```
## <a name="say-as-element"></a>say-as-Element

`say-as` ist ein optionales Element, das den Inhaltstyp (z.¬†B. Zahl oder Datum) f√ºr den Text des Elements angibt. Es informiert die Sprachsynthese-Engine, wie der Text ausgesprochen wird.

**Syntax**

```XML
<say-as interpret-as="string" format="digit string" detail="string"> <say-as>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `interpret-as` | Gibt an, welchen Inhaltstyp der Text des Elements darstellt. Die folgende Tabelle listet die unterschiedlichen Typen auf. | Erforderlich |
| `format` | Enth√§lt weitere Informationen, wie genau der Elementtext formatiert ist, f√ºr Inhaltstypen, die mehrdeutige Formate haben k√∂nnen. SSML definiert Formate f√ºr Inhaltstypen, die diese verwenden (siehe Tabelle unten). | Optional |
| `detail` | Gibt die Menge der auszusprechenden Details an. Dieses Attribut k√∂nnte beispielsweise bei der Sprachsynthese-Engine die Aussprache von Satzzeichen anfordern. F√ºr `detail` sind keine Standardwerte definiert. | Optional |

<!-- I don't understand the last sentence. Don't we know which one Cortana uses? -->

Im Folgenden finden Sie die unterst√ºtzten Inhaltstypen f√ºr die Attribute `interpret-as` und `format`. F√ºgen Sie das `format`-Attribut nur dann ein, wenn `interpret-as` auf Datum und Uhrzeit festgelegt ist.

| interpret-as | format | Interpretation |
|--------------|--------|----------------|
| `address` | | Der Text wird als Adresse ausgesprochen. Aussprache der Sprachsynthese-Engine:<br /><br />`I'm at <say-as interpret-as="address">150th CT NE, Redmond, WA</say-as>`<br /><br />Als ‚ÄûIch bin am 150th Court Nordost Redmond Washington‚Äú. |
| `cardinal`, `number` | | Der Text wird als Kardinalzahl ausgesprochen. Aussprache der Sprachsynthese-Engine:<br /><br />`There are <say-as interpret-as="cardinal">3</say-as> alternatives`<br /><br />Als ‚ÄûEs gibt drei Alternativen.‚Äú |
| `characters`, `spell-out` | | Der Text wird als einzelner Buchstabe (buchstabiert) ausgesprochen. Aussprache der Sprachsynthese-Engine:<br /><br />`<say-as interpret-as="characters">test</say-as>`<br /><br />Als ‚ÄûT E S T.‚Äú |
| `date` | TMJ, MTJ, JMT, JTM, JM, MT, TM, T, M, J | Der Text wird als Datum ausgesprochen. Das `format`-Attribut gibt das Datumsformat an (*T = Tag, M = Monat und J = Jahr*). Aussprache der Sprachsynthese-Engine:<br /><br />`Today is <say-as interpret-as="date" format="mdy">10-19-2016</say-as>`<br /><br />Als ‚Äûheute ist der neunzehnte Oktober zweitausendsechzehn‚Äú. |
| `digits`, `number_digit` | | Der Text wird als Sequenz einzelner Ziffern gesprochen. Aussprache der Sprachsynthese-Engine:<br /><br />`<say-as interpret-as="number_digit">123456789</say-as>`<br /><br />Als ‚Äû1 2 3 4 5 6 7 8 9‚Äú. |
| `fraction` | | Der Text wird als Bruchzahl ausgesprochen. Aussprache der Sprachsynthese-Engine:<br /><br /> `<say-as interpret-as="fraction">3/8</say-as> of an inch`<br /><br />Als ‚Äûdrei achtel Zoll‚Äú. |
| `ordinal` | | Der Text wird als Ordinalzahl ausgesprochen. Aussprache der Sprachsynthese-Engine:<br /><br />`Select the <say-as interpret-as="ordinal">3rd</say-as> option`<br /><br />Als ‚ÄûW√§hlen Sie die dritte Option aus.‚Äú |
| `telephone` | | Der Text wird als Telefonnummer ausgesprochen. Das Attribut `format` kann Ziffern enthalten, die eine Landeskennzahl darstellen. Beispiel: ‚Äû1‚Äú f√ºr die USA oder ‚Äû39‚Äú f√ºr Italien. Die Sprachsynthese-Engine kann sich anhand dieser Informationen orientieren, wie eine Telefonnummer auszusprechen ist. Wenn die Telefonnummer ebenfalls die Landeskennzahl enth√§lt, hat diese Vorrang vor der Landeskennzahl in `format`. Aussprache der Sprachsynthese-Engine:<br /><br />`The number is <say-as interpret-as="telephone" format="1">(888) 555-1212</say-as>`<br /><br />Als ‚ÄûMeine Nummer lautet Vorwahl acht acht acht f√ºnf f√ºnf f√ºnf eins zwei eins zwei.‚Äú |
| `time` | hms12, hms24 | Der Text wird als Uhrzeit ausgesprochen. Das `format`-Attribut gibt an, ob die Uhrzeit im 12-Stunden-Format (hms12) oder 24-Stunden-Format (hms24) angegeben wird. Verwenden Sie einen Doppelpunkt zum Trennen von Zahlen, die Stunden, Minuten und Sekunden darstellen. Beispielsweise ist Folgendes zul√§ssig: 12:35, 1:14:32, 08:15 und 02:50:45. Aussprache der Sprachsynthese-Engine:<br /><br />`The train departs at <say-as interpret-as="time" format="hms12">4:00am</say-as>`<br /><br />Als ‚ÄûDer Zug f√§hrt um vier Uhr morgens.‚Äú |

**Verwendung**

Das `say-as`-Element kann nur Text enthalten.

**Beispiel**

Die Sprachsynthese-Engine spricht den Beispielsatz wie folgt aus: ‚ÄûIhre erste Anfrage war f√ºr ein Zimmer am neunzehnten Oktober zweitausendzehn mit fr√ºhzeitiger Ankunft um zw√∂lf Uhr f√ºnfunddrei√üig nachmittags‚Äú.

```XML
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <p>
        Your <say-as interpret-as="ordinal"> 1st </say-as> request was for <say-as interpret-as="cardinal"> 1 </say-as> room
        on <say-as interpret-as="date" format="mdy"> 10/19/2010 </say-as>, with early arrival at <say-as interpret-as="time" format="hms12"> 12:35pm </say-as>.
        </p>
    </voice>
</speak>
```

## <a name="add-recorded-audio"></a>Hinzuf√ºgen von Audioaufzeichnungen

`audio` ist ein optionales Element, mit dem Sie MP3-Audioaufzeichnungen in ein SSML-Dokument einf√ºgen k√∂nnen. Der Text des Elements ‚Äûaudio‚Äú kann Nur-Text oder SSML-Markup enthalten, das verwendet wird, wenn die Audiodatei nicht verf√ºgbar oder nicht abspielbar ist. Au√üerdem kann das Element `audio` Text und die folgenden Elemente enthalten: `audio`, `break`, `p`, `s`, `phoneme`, `prosody`, `say-as` und `sub`.

Alle Audiodaten, die im SSML-Dokument enthalten sind, m√ºssen die folgenden Anforderungen erf√ºllen:

* Die MP3-Datei muss auf einem HTTPS-Endpunkt gehostet werden, der √ºber das Internet zug√§nglich ist. HTTPS ist erforderlich, und die Dom√§ne, die die MP3-Datei hostet, muss √ºber ein g√ºltiges vertrauensw√ºrdiges TSL/SSL-Zertifikat verf√ºgen.
* Es muss es sich um eine g√ºltige MP3-Datei (MPEG¬†v2) handeln.
* Die Bitrate muss 48¬†KBit/s betragen.
* Die Abtastrate muss bei 16.000¬†Hz liegen.
* Die Gesamtzeit f√ºr alle Text- und Audiodateien in einer einzelnen Antwort darf nicht √ºber 90¬†Sekunden liegen.
* Die MP3-Datei darf keine kundenspezifischen oder andere vertrauliche Informationen enthalten.

**Syntax**

```xml
<audio src="string"/></audio>
```

**Attribute**

| attribute | BESCHREIBUNG                                   | Erforderlich/optional                                        |
|-----------|-----------------------------------------------|------------------------------------------------------------|
| `src`     | Gibt den Speicherort bzw. die URL der Audiodatei an. | Erforderlich, wenn Sie das Element ‚Äûaudio‚Äú im SSML-Dokument verwenden. |

**Beispiel**

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-JennyNeural">
        <p>
            <audio src="https://contoso.com/opinionprompt.wav"/>
            Thanks for offering your opinion. Please begin speaking after the beep.
            <audio src="https://contoso.com/beep.wav">
                Could not play the beep, please voice your opinion now.
            </audio>
        </p>
    </voice>
</speak>
```

## <a name="add-background-audio"></a>Hinzuf√ºgen von Hintergrundaudioaufnahmen

Das Element `mstts:backgroundaudio` erm√∂glicht es Ihnen, Hintergrundaudioaufnahmen zu Ihren SSML-Dokumenten hinzuzuf√ºgen (oder eine Audiodatei mit Text-zu-Sprache zu vermischen). Mithilfe von `mstts:backgroundaudio` k√∂nnen Sie im Hintergrund eine Audiodatei in Dauerschleife abspielen, die am Anfang der Text-zu-Sprache-Aufnahme ein und am Ende wieder ausgeblendet wird.

Wenn die bereitgestellte Hintergrundaudiospur k√ºrzer ist als die Text-zu-Sprache-Aufnahme oder das Ausblenden, wird diese wieder von vorne abgespielt. Wenn sie l√§nger als die Text-zu-Sprache-Aufnahme ist, wird sie angehalten, sobald der Ausblendevorgang abgeschlossen ist.

Pro SSML-Dokument ist nur eine Hintergrundaudiodatei zul√§ssig. Sie k√∂nnen jedoch `audio`-Tags in das Element `voice` integrieren, um dem SSML-Dokument zus√§tzliche Audioaufnahmen hinzuzuf√ºgen.

**Syntax**

```XML
<mstts:backgroundaudio src="string" volume="string" fadein="string" fadeout="string"/>
```

**Attribute**

| attribute | BESCHREIBUNG | Erforderlich/optional |
|-----------|-------------|---------------------|
| `src` | Gibt den Speicherort bzw. die URL der Hintergrundaudiodatei an. | Erforderlich, wenn Sie eine Hintergrundaudioaufnahme in Ihrem SSML-Dokument verwenden. |
| `volume` | Gibt die Lautst√§rke der Hintergrundaudiodatei an. **Akzeptierte Werte**: `0` bis `100` (einschlie√ülich). Standardwert: `1`. | Optional |
| `fadein` | Gibt (in Millisekunden) an, wie lange die Hintergrundaudiodatei eingeblendet wird. Der Standardwert ist `0`, was dem ‚ÄûNicht einblenden‚Äú entspricht. **Akzeptierte Werte**: `0` bis `10000` (einschlie√ülich).  | Optional |
| `fadeout` | Gibt (in Millisekunden) an, wie lange die Hintergrundaudiodatei ausgeblendet wird. Der Standardwert ist `0`, was dem ‚ÄûNicht ausblenden‚Äú entspricht. **Akzeptierte Werte**: `0` bis `10000` (einschlie√ülich).  | Optional |

**Beispiel**

```xml
<speak version="1.0" xml:lang="en-US" xmlns:mstts="http://www.w3.org/2001/mstts">
    <mstts:backgroundaudio src="https://contoso.com/sample.wav" volume="0.7" fadein="3000" fadeout="4000"/>
    <voice name="Microsoft Server Speech Text to Speech Voice (en-US, JennyNeural)">
        The text provided in this document will be spoken over the background audio.
    </voice>
</speak>
```

## <a name="bookmark-element"></a>Lesezeichen-Element

Mit dem Lesezeichen-Element k√∂nnen Sie benutzerdefinierte Marker in SSML einf√ºgen, um den Offset der einzelnen Marker im Audiostream zu erhalten.
Die Lesezeichen-Elemente werden nicht gelesen.
Das Lesezeichen-Element kann verwendet werden, um auf eine bestimmte Position in der Text-oder Tag-Sequenz zu verweisen.

> [!NOTE]
> `bookmark` Das Element funktioniert zurzeit nur f√ºr `en-US-AriaNeural` Voice.

**Syntax**

```xml
<bookmark mark="string"/>
```

**Attribute**

| attribute | BESCHREIBUNG                                   | Erforderlich/optional                                        |
|-----------|-----------------------------------------------|------------------------------------------------------------|
|  `mark`   | Gibt den Verweistext des `bookmark` Elements an. | Erforderlich. |

**Beispiel**

Beispielsweise k√∂nnen Sie den Zeitversatz (Offset) jedes Blumenworts wie folgt ermitteln:

```xml
<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
    <voice name="en-US-AriaNeural">
        We are selling <bookmark mark='flower_1'/>roses and <bookmark mark='flower_2'/>daisies.
    </voice>
</speak>
```

### <a name="get-bookmark-using-speech-sdk"></a>Lesezeichen mit Sprach-SDK aufrufen

Sie k√∂nnen das `BookmarkReached` Ereignis in Sprach-SDK abonnieren, um die Lesezeichen-Offsets zu erhalten.

> [!NOTE]
> Das `BookmarkReached`-Ereignis ist erst ab Speech SDK Version 1.16 verf√ºgbar.

Ereignisse vom Typ `BookmarkReached` werden ausgel√∂st, wenn die ausgegebenen Audiodaten verf√ºgbar werden, was schneller passiert als die Wiedergabe √ºber ein Ausgabeger√§t.

* `AudioOffset` meldet die verstrichene Zeit der Audioausgabe zwischen dem Beginn der Synthese und dem Lesezeichen-Element. Dieser Wert wird in HNS-Einheiten (hundert Nanosekunden) gemessen. 10.000¬†HNS entsprechen einer Millisekunde.
* `Text` ist der Verweistext des Lesezeichen-Elements, bei dem es sich um die Zeichenfolge handelt, die im Attribut `mark` festgelegt wird.

# <a name="c"></a>[C#](#tab/csharp)

Weitere Informationen finden Sie unter <a href="/dotnet/api/microsoft.cognitiveservices.speech.speechsynthesizer.bookmarkreached" target="_blank"> `BookmarkReached` </a>.

```csharp
synthesizer.BookmarkReached += (s, e) =>
{
    // The unit of e.AudioOffset is tick (1 tick = 100 nanoseconds), divide by 10,000 to convert to milliseconds.
    Console.WriteLine($"Bookmark reached. Audio offset: " +
        $"{e.AudioOffset / 10000}ms, bookmark text: {e.Text}.");
};
```

F√ºr das obige SSML-Beispiel wird das `BookmarkReached` Ereignis zweimal ausgel√∂st, und die Konsolenausgabe ist
```text
Bookmark reached. Audio offset: 825ms, bookmark text: flower_1.
Bookmark reached. Audio offset: 1462.5ms, bookmark text: flower_2.
```

# <a name="c"></a>[C++](#tab/cpp)

Weitere Informationen finden Sie unter <a href="/cpp/cognitive-services/speech/speechsynthesizer#bookmarkreached" target="_blank"> `BookmarkReached` </a>.

```cpp
synthesizer->BookmarkReached += [](const SpeechSynthesisBookmarkEventArgs& e)
{
    cout << "Bookmark reached. "
        // The unit of e.AudioOffset is tick (1 tick = 100 nanoseconds), divide by 10,000 to convert to milliseconds.
        << "Audio offset: " << e.AudioOffset / 10000 << "ms, "
        << "bookmark text: " << e.Text << "." << endl;
};
```

F√ºr das obige SSML-Beispiel wird das `BookmarkReached` Ereignis zweimal ausgel√∂st, und die Konsolenausgabe ist
```text
Bookmark reached. Audio offset: 825ms, bookmark text: flower_1.
Bookmark reached. Audio offset: 1462.5ms, bookmark text: flower_2.
```

# <a name="java"></a>[Java](#tab/java)

Weitere Informationen finden Sie unter <a href="/java/api/com.microsoft.cognitiveservices.speech.speechsynthesizer.bookmarkReached#com_microsoft_cognitiveservices_speech_SpeechSynthesizer_BookmarkReached" target="_blank"> `BookmarkReached` </a>.

```java
synthesizer.BookmarkReached.addEventListener((o, e) -> {
    // The unit of e.AudioOffset is tick (1 tick = 100 nanoseconds), divide by 10,000 to convert to milliseconds.
    System.out.print("Bookmark reached. Audio offset: " + e.getAudioOffset() / 10000 + "ms, ");
    System.out.println("bookmark text: " + e.getText() + ".");
});
```

F√ºr das obige SSML-Beispiel wird das `BookmarkReached` Ereignis zweimal ausgel√∂st und die Konsolenausgabe ist
```text
Bookmark reached. Audio offset: 825ms, bookmark text: flower_1.
Bookmark reached. Audio offset: 1462.5ms, bookmark text: flower_2.
```

# <a name="python"></a>[Python](#tab/python)

Weitere Informationen finden Sie unter <a href="/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechsynthesizer#bookmark-reached" target="_blank"> `bookmark_reached` </a>.

```python
# The unit of evt.audio_offset is tick (1 tick = 100 nanoseconds), divide it by 10,000 to convert to milliseconds.
speech_synthesizer.bookmark_reached.connect(lambda evt: print(
    "Bookmark reached: {}, audio offset: {}ms, bookmark text: {}.".format(evt, evt.audio_offset / 10000, evt.text)))
```

F√ºr das obige SSML-Beispiel wird das `bookmark_reached` Ereignis zweimal ausgel√∂st und die Konsolenausgabe ist
```text
Bookmark reached, audio offset: 825ms, bookmark text: flower_1.
Bookmark reached, audio offset: 1462.5ms, bookmark text: flower_2.
```

# <a name="javascript"></a>[JavaScript](#tab/javascript)

Weitere Informationen finden Sie unter <a href="/javascript/api/microsoft-cognitiveservices-speech-sdk/speechsynthesizer#bookmarkReached" target="_blank"> `bookmarkReached`</a>.

```javascript
synthesizer.bookmarkReached = function (s, e) {
    window.console.log("(Bookmark reached), Audio offset: " + e.audioOffset / 10000 + "ms, bookmark text: " + e.text);
}
```

F√ºr das obige SSML-Beispiel wird das `bookmarkReached` Ereignis zweimal ausgel√∂st und die Konsolenausgabe ist
```text
(Bookmark reached), Audio offset: 825ms, bookmark text: flower_1.
(Bookmark reached), Audio offset: 1462.5ms, bookmark text: flower_2.
```

# <a name="objective-c"></a>[Objective-C](#tab/objectivec)

Weitere Informationen finden Sie unter <a href="/objectivec/cognitive-services/speech/spxspeechsynthesizer#addbookmarkreachedeventhandler" target="_blank"> `addBookmarkReachedEventHandler` </a>.

```objectivec
[synthesizer addBookmarkReachedEventHandler: ^ (SPXSpeechSynthesizer *synthesizer, SPXSpeechSynthesisBookmarkEventArgs *eventArgs) {
    // The unit of AudioOffset is tick (1 tick = 100 nanoseconds), divide by 10,000 to converted to milliseconds.
    NSLog(@"Bookmark reached. Audio offset: %fms, bookmark text: %@.", eventArgs.audioOffset/10000., eventArgs.text);
}];
```

F√ºr das obige SSML-Beispiel wird das `BookmarkReached` Ereignis zweimal ausgel√∂st und die Konsolenausgabe ist
```text
Bookmark reached. Audio offset: 825ms, bookmark text: flower_1.
Bookmark reached. Audio offset: 1462.5ms, bookmark text: flower_2.
```

# <a name="swift"></a>[Swift](#tab/swift)

Weitere Informationen finden Sie unter <a href="/objectivec/cognitive-services/speech/spxspeechsynthesizer" target="_blank"> `addBookmarkReachedEventHandler` </a>.

---

## <a name="next-steps"></a>N√§chste Schritte

* [Sprachunterst√ºtzung: Stimmen, Gebietsschemas, Sprachen](language-support.md)
